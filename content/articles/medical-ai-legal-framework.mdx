---
title: "医療AIと法的責任 - 2026年版 医師・病院が知るべき規制フレームワーク"
description: "AIを使った診療で何かが起きたとき、誰が責任を取るのか。薬機法・個人情報保護法・PMDAガイドラインから実践的なコンプライアンス対策まで解説します"
category: "ai-fundamentals"
contentType: "article"
difficulty: "intermediate"
tags: ["法的責任", "規制", "PMDA", "薬機法", "個人情報", "コンプライアンス", "SaMD"]
publishedAt: "2026-02-24"
featured: true
---

# 医療AIと法的責任

## はじめに：「AI が言ったから」は通らない

ある外来診療のシナリオを考えてみましょう。AIが提示した鑑別診断リストを参考に治療方針を決定したところ、診断が誤っていて患者に有害事象が発生した。この場合、責任はAI開発企業にあるのでしょうか。それとも医師にあるのでしょうか。

答えは明確です。**現行の法体系において、診療の最終責任は医師にあります。** AIはあくまでも「ツール」であり、その出力を使って判断した医師が責任を負います。

本記事では、医療AIを取り巻く法的・規制的フレームワークを整理し、医師・病院として何を理解し、どう行動すべきかを解説します。

---

## 第1章：医師の法的責任とAI

### 診療責任の原則

日本の医療法・医師法において、診療行為は医師が行うものと定義されています。AIを使った診療においても、この原則は変わりません。

**医師に求められる「医療水準」**

医療過誤訴訟では「当時の医療水準に照らして適切な診療を行ったか」が問われます。AIが普及した現代では、「AIを適切に活用しなかったこと」が医療水準を下回ると判断されるケースが今後増える可能性があります。逆に、「AIを使ったが出力を無批判に採用した」という事実も問題になり得ます。

### AIエラーが発生した場合の責任分担

現時点では以下のように責任が分散しています：

| 当事者 | 責任の範囲 |
|--------|------------|
| 医師 | 診療判断の最終責任、AIの適切な使用・監督 |
| 病院・施設 | AI導入の判断、使用ポリシーの整備、スタッフ教育 |
| AI開発企業 | 製品の安全性・有効性の担保（薬機法規制の対象） |
| 医療機器販売業者 | 適切な情報提供、アフターサポート |

**重要な判断基準**：「医師が適切な監督のもとでAIを使用したか」が問われます。AIの出力を盲目的に採用することは、医師の判断放棄とみなされかねません。

---

## 第2章：薬機法とAI医療機器（SaMD）

### プログラム医療機器（SaMD）とは

「Software as a Medical Device（SaMD）」は、医療機器として機能するソフトウェアを指します。日本では薬機法（医薬品、医療機器等の品質、有効性及び安全性の確保等に関する法律）の規制対象となります。

**SaMDに該当するAIの例**：

- 胸部CTから肺がんを自動検出するシステム
- 心電図から心房細動を診断するアプリ
- 眼底画像から糖尿病網膜症を検出するソフト

**SaMDに該当しない可能性が高いもの**：

- 医師の入力を受けて参考情報を提示するだけのLLMアシスタント（ただしグレーゾーンあり）
- 文書作成補助ツール
- 患者教育コンテンツ生成ツール

### リスク分類と規制レベル

薬機法ではSaMDをリスクに応じて分類しています：

**クラスⅠ（届出）**: 疾患に関係するが、診断・治療の最終判断に直接影響しないソフト

**クラスⅡ（第三者認証）**: 中程度のリスク。診断支援を行うが、医師の確認が前提のもの

**クラスⅢ（承認）**: 高リスク。生命維持装置や植込み機器のソフトウェア

**クラスⅣ（承認）**: 最高リスク。自動診断・治療決定を行うシステム

ChatGPTやClaudeをそのまま使う「汎用LLM」は現時点では医療機器に該当しないとされています（PMDAの見解）。しかし、これを特定の診断目的に特化してシステム化・販売する場合は規制対象になる可能性があります。

---

## 第3章：個人情報保護法と医療データ

### 医療情報は「要配慮個人情報」

個人情報保護法では、医療・健康情報は「要配慮個人情報」として特別な保護が義務付けられています。取得・利用・提供のすべてにおいて、通常の個人情報より厳しいルールが適用されます。

**AIに患者情報を入力する際のリスク**

| リスク | 内容 |
|--------|------|
| データの外部送信 | クラウドAIでは入力データが海外サーバーに送信される場合がある |
| 第三者提供の問題 | 患者の同意なしにAI企業にデータが渡ることになりかねない |
| 学習データへの利用 | 一部サービスでは入力データがモデル改善に使われる可能性 |
| セキュリティインシデント | AIサービスへの不正アクセスによる情報漏洩リスク |

### 法令遵守のための匿名化

最も確実な対策は**氏名・生年月日・患者IDなどの直接識別子を除去する匿名化処理**です。ただし「匿名加工情報」の基準は個人情報保護法で厳密に定義されており、単に氏名を削除しただけでは不十分な場合があります。

**実践的な匿名化プロセス**：

1. **直接識別子の除去**: 氏名、生年月日、住所、患者ID、電話番号、メールアドレス
2. **間接識別子のリスク評価**: 特殊な疾患名、珍しい治療歴の組み合わせは特定につながる可能性
3. **年齢の範囲化**: 正確な年齢の代わりに年代（「50代男性」等）を使用
4. **地名の一般化**: 具体的な住所の代わりに都道府県レベルに留める

---

## 第4章：PMDAとAI規制の現状

### PMDAのAI対応ワーキンググループ

PMDA（独立行政法人医薬品医療機器総合機構）は2024年末に「AI対応ワーキンググループ（WG）」を設置し、AI医療機器の審査・規制について専門的な議論を進めています。

**現在の主要議題**：

1. **継続的学習（Continual Learning）の扱い**: 承認後にAIモデルが更新される場合、どこまでが「変更承認不要」の範囲か
2. **LLMと医療機器の境界**: 汎用LLMを医療目的に使う場合の規制上の扱い
3. **バリデーション方法**: AIの性能評価に関する標準的な方法論の策定
4. **リアルワールドデータの活用**: 承認後のモニタリングにおけるRWD活用方針

**医師への影響**：PMDA承認を受けたAI医療機器と、承認を受けていない汎用AIの使い分けが今後より明確に求められるようになります。

### SaMD審査の優先審査制度

PMDAは革新的なAI医療機器に対する「優先審査」制度を設けています。対象となる条件は：

- 生命に重大な影響を与える疾患の診断・治療に関わるもの
- 既存の治療法と比較して著明な有効性が見込まれるもの
- 医療上の必要性が特に高いもの

---

## 第5章：EU AI Act — 欧州展開の場合

### High-Risk AI Systemとしての医療AI

EU AI Actが完全適用される2026年以降、欧州市場向けの医療AIはより厳しい要件への対応が必要です。

**医療AI（診断支援系）に適用される主要要件**：

- **リスク評価と軽減措置**: 体系的なリスク評価プロセスの実施と文書化
- **データガバナンス**: 学習・検証・テストデータの品質管理と文書化
- **透明性**: AIシステムの能力と限界の明確な開示（患者・医師向け）
- **人間による監督**: 最終判断の人間への担保
- **精度・堅牢性・サイバーセキュリティ**: 継続的な性能モニタリング
- **CE-Markとの整合**: 既存のMDR（医療機器規則）との二重要件への対応

日本企業が欧州市場に展開する際は、このEU AI Act対応が製品設計の段階から必要になります。

---

## 第6章：病院・施設が整備すべきこと

### AIガバナンス体制の構築

医療機関がAIを安全に導入・運用するために必要な体制：

**1. AI使用ポリシーの策定**

最低限含めるべき内容：

- 院内で使用を許可するAIサービスのリスト（ホワイトリスト）
- 患者情報の匿名化ルール
- AI出力の検証・確認手順
- インシデント発生時の報告フロー

**2. 教育・研修プログラム**

- 全スタッフへのAI基礎リテラシー教育（年1回以上推奨）
- 各職種に特化したAI活用スキルトレーニング
- AIエラー事例の共有と振り返り

**3. モニタリングと評価**

- AI使用状況のログ記録
- アウトカムへの影響評価（AI使用前後の比較）
- 定期的なツール評価と見直し

### インフォームドコンセントの整備

患者にAI支援を使用している旨を説明することは、今後の標準的慣行になる可能性があります。

**説明のポイント**：
- AIはあくまで医師の判断を支援するツールであること
- 最終的な診断・治療判断は医師が行うこと
- AIの使用がプライバシーに与える影響（匿名化処理済みであること等）

---

## 第7章：医師個人として実践すべきこと

### 日常診療におけるコンプライアンスチェックリスト

```
□ 患者情報を入力する際、氏名・生年月日等の識別子を除去したか
□ 使用するAIサービスが院内ポリシーで許可されているか確認したか
□ AI出力を一次資料（ガイドライン・添付文書等）で検証したか
□ AIの提案を最終判断の参考としてのみ使い、盲目的に採用していないか
□ AIの使用が診療録に適切に記録されているか（施設ルールに従う）
□ AIが提示した論文・参考文献の実在をPubMedで確認したか
```

### 「AIを使ったこと」の記録

現時点では診療録へのAI使用記載義務は明確ではありませんが、将来の法的紛争リスクを考えると、重要な判断に際してAIを参照した場合は記録しておくことが合理的です。

**記録例**：「LLM（Claude）を参照し鑑別診断リストを検討。最終的に臨床所見および〇〇検査結果をもとに△△と診断した。」

---

## まとめ：3つの原則

医療AIの法的フレームワークは現在進行形で整備されています。確実に言えることは：

**原則1: 最終責任は医師にある**
AIの出力は参考情報に留め、診療判断の主体は常に医師であるという認識を持つ。

**原則2: 患者情報保護は非交渉**
個人情報保護法・院内ポリシーに従った匿名化処理は、AIを使う前の必須ステップ。

**原則3: 記録と検証を習慣に**
AI出力の根拠を確認する習慣と、使用状況の適切な記録が、将来の法的リスクを最小化する。

医療AIは「使うかどうか」の問いから「どう安全に使うか」の問いへ移行しています。法的・倫理的フレームワークを理解したうえで、積極的にAIを活用していく姿勢が求められています。

---

## 参考リンク

- [PMDA AI対応ワーキンググループ](https://www.pmda.go.jp/rs-std-jp/cross-sectional-project/0024.html)
- [厚生労働省 医療機器プログラム（SaMD）に関する情報](https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000060395.html)
- [個人情報保護委員会 医療・介護関係事業者向けガイダンス](https://www.ppc.go.jp/personalinfo/legal/guidelines_health/)

---

更新日: 2026年2月24日
カテゴリ: AI基礎
難易度: 中級
所要時間: 約20分（通読）
