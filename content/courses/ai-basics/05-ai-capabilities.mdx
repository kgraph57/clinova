---
title: "AIの能力と限界"
description: "パターン認識の強み、ハルシネーション、バイアス、説明可能なAI（XAI）の課題を具体事例で学びます"
order: 5
estimatedMinutes: 15
---

# AIの能力と限界

## はじめに — AIは皮膚科専門医に勝ったのか？

2017年、スタンフォード大学の研究チームがNature誌に画期的な論文を発表しました。深層学習モデルが、皮膚がんの画像診断で**21名の皮膚科専門医と同等以上**の精度を達成したのです。しかし、この「同等以上」には重要な但し書きがあります。AIはデジタル画像でのみ評価され、実際の患者の皮膚を見たわけではありません。触診、患者の病歴、ダーモスコピーの動的所見は一切含まれていません。

この事例は、AIの能力と限界を象徴しています。**特定の条件下では人間を超える**が、**現実の臨床環境は特定の条件ではない**。

<ResourceCard
  type="paper"
  title="Dermatologist-level classification of skin cancer with deep neural networks"
  url="https://www.nature.com/articles/nature21056"
  authors="Esteva A, Kuprel B, Novoa RA et al."
  source="Nature"
  year={2017}
  description="深層学習が皮膚がん診断で皮膚科専門医と同等精度を達成した画期的論文"
/>

---

## AIが得意なこと — 3つの強み

### 1. パターン認識 — 人間が見落とすものを見つける

<CaseStudy title="EIRL — 脳動脈瘤の見逃しを9ポイント改善" year={2019} jurisdiction="日本" tags={["画像診断", "脳動脈瘤", "感度向上"]}>

**課題**: 脳MRI読影における脳動脈瘤の見逃し率は放射線科医でも約30%。特に3mm未満の小さな動脈瘤は見落とされやすい。

**成果**: LPIXEL社のEIRLは、AI支援なしの読影感度68.2%を、AI支援ありで**77.2%に向上**させた。特に小さな動脈瘤の検出で改善が顕著。

**原理**: AIは画像の全ピクセルを均等に評価する。人間は疲労や注意力の低下で見逃しが起きるが、AIは一貫した精度を保つ。

</CaseStudy>

### 2. 大量データの高速処理 — 人間にはできないスケール

AIは数千枚の画像を数分で処理できます。放射線科医が1枚のCT画像を読影するのに平均5-10分かかるのに対し、AIは数秒で全スライスをスキャンし、異常候補を提示できます。

### 3. 一貫性 — 疲れない、揺れない

人間の医師の判断は、疲労、時間帯、直前の症例の影響を受けます（アンカリングバイアス）。AIは同じ入力に対して常に同じ出力を返し、判断の一貫性を保ちます。

---

## AIが苦手なこと — 4つの限界

### 1. ハルシネーション — 「嘘を自信満々に語る」

<CaseStudy title="ChatGPTのハルシネーション — 医療情報での危険性" year={2023} jurisdiction="国際" tags={["ハルシネーション", "生成AI", "医療安全"]}>

**現象**: 大規模言語モデル（LLM）は、訓練データに存在しない情報を、あたかも事実であるかのように生成することがある。これを**ハルシネーション（幻覚）**と呼ぶ。

**医療での具体例**:
- 存在しない臨床試験の結果を引用
- 実在しない医学論文の著者・タイトル・ジャーナルを生成
- 薬剤の用量を誤って提示（桁違いの投与量を生成する事例が報告）

**根本原因**: LLMは「次に来る確率が高い単語」を予測しているだけであり、医学的な真偽を判断する能力を持っていない。文脈上「もっともらしい」回答を生成するが、それが事実かどうかは別の問題。

**対策**: AIが生成した医療情報は、必ず一次資料（原著論文、添付文書、ガイドライン）で確認する。AIの出力を「下書き」として扱い、最終確認は人間が行う。

</CaseStudy>

### 2. 文脈理解の限界 — 「行間を読めない」

AIはテキストや画像のパターンを処理しますが、人間の医師のような「文脈の統合」はできません。

| 人間の医師ができること | AIの限界 |
|:---|:---|
| 患者の表情・声色から不安を読み取る | テキスト・数値データのみ処理 |
| 「少し痛い」の背後にある重症の可能性を直感で察知 | 入力された文字列のみを処理 |
| 家族構成・経済状況を考慮した治療計画 | 与えられたデータ項目のみで判断 |
| 過去の臨床経験から稀少疾患を想起 | 訓練データにない疾患は検出不能 |

### 3. バイアス — 「データの偏りをそのまま学習する」

<CaseStudy title="皮膚科AIの肌色バイアス — 公平性の課題" year={2021} jurisdiction="国際" tags={["バイアス", "公平性", "皮膚科"]}>

**問題**: 皮膚科AIの訓練データの大部分が白人患者の画像で構成されていたため、有色人種の皮膚病変の検出精度が低いことが複数の研究で報告された。

**具体的な格差**: ある研究では、白人の皮膚がん検出感度が90%以上だった一方、暗い肌色での感度は70%台に低下。

**原因**: AIは訓練データの分布を忠実に学習する。訓練データに特定の集団が少なければ、その集団に対する性能が低下する。これは技術的な限界ではなく、データ収集の偏りという社会的問題。

**教訓**: AI医療機器を評価する際は、「どのような患者集団で検証されたか」を必ず確認する。自施設の患者層と検証データの集団が異なる場合、公表された精度がそのまま適用できない可能性がある。

</CaseStudy>

### 4. ブラックボックス問題 — 「なぜそう判断したか説明できない」

<Callout type="insight" title="説明可能なAI（XAI）— ブラックボックスを開く試み">

**問題**: 深層学習は高精度だが、「なぜその判断をしたか」を説明できない。医療では「AIが異常と判定しました」だけでは不十分で、「画像のどの部分をどう評価した結果か」の説明が求められる。

**代表的な手法**:
- **Grad-CAM**: 画像のどの領域がAIの判定に影響したかをヒートマップで可視化
- **SHAP**: 各特徴量が予測にどの程度寄与したかを数値化
- **Attention Map**: Transformerモデルが「注目した」部分を可視化

**医療での活用**: EIRLは脳動脈瘤の候補位置をマーキングして表示し、医師が「AIがどこを見ているか」を確認できる。完全な説明ではないが、臨床判断の補助として有用。

</Callout>

---

## 能力と限界の実践的整理

<Callout type="comparison" title="AIに任せるべきこと vs 人間が行うべきこと">

**AIに適するタスク**:
- 大量画像のスクリーニング（見逃しリスクの低減）
- 定型文書のドラフト作成（時間の効率化）
- 検査値の異常パターンの検出（24時間の一貫した監視）
- 文献検索と要約（情報収集の加速）

**人間が不可欠なタスク**:
- 最終診断と治療方針の決定（責任と文脈判断）
- 患者・家族とのコミュニケーション（共感と信頼）
- 倫理的判断（治療の中止・継続の決定）
- AIの出力の検証（ハルシネーション・バイアスの確認）

→ **原則**: AIは「第二の意見」として活用し、最終判断は常に医師が行う。

</Callout>

---

## まとめ

AIは画像認識や大量データ処理など「パターンマッチング」が求められるタスクで人間を超える能力を示す一方、ハルシネーション、文脈理解の限界、データバイアス、ブラックボックス問題という根本的な限界を抱えています。スタンフォード大学の皮膚がんAIの事例が示すように、「特定条件下での優秀さ」と「臨床現場での信頼性」は異なります。AIの出力を批判的に評価し、人間の判断と組み合わせることが適切な活用の鍵です。

次のレッスンでは、AIの**現状と医療分野での具体的な展望**を学びます。

<Quiz
  courseId="ai-basics"
  lessonSlug="05-ai-capabilities"
  questions={[
    {
      question: "AIのハルシネーション（幻覚）とはどのような現象か？",
      options: ["AIの処理速度が低下する現象", "AIが訓練データに存在しない情報を事実であるかのように生成する現象", "AIが画像を正しく認識できない現象", "AIの学習が途中で停止する現象"],
      correctIndex: 1,
      explanation: "ハルシネーションとは、大規模言語モデルが訓練データに存在しない情報（存在しない論文、誤った薬剤用量など）を、あたかも事実であるかのように自信を持って生成する現象です。LLMは「次に来る確率が高い単語」を予測しているだけで、医学的真偽を判断できないため起こります。"
    },
    {
      question: "皮膚科AIのバイアス問題が示す教訓として最も重要なものはどれか？",
      options: ["AIは皮膚科には使えない", "AI医療機器を評価する際は、検証データの患者集団と自施設の患者層の違いを確認すべき", "AIのバイアスは技術的に解決不可能である", "訓練データの量を増やせばバイアスは自然に解消される"],
      correctIndex: 1,
      explanation: "皮膚科AIの肌色バイアスは、訓練データが白人患者に偏っていたことが原因です。AI医療機器を評価する際は「どのような患者集団で検証されたか」を確認し、自施設の患者層と異なる場合は公表精度がそのまま適用できない可能性を考慮すべきです。"
    },
    {
      question: "説明可能なAI（XAI）が医療現場で重要な理由はどれか？",
      options: ["AIの処理速度を向上させるため", "医師がAIの判断根拠を確認し、臨床判断の補助として活用するため", "AIの開発コストを削減するため", "患者にAIの技術的仕組みを説明するため"],
      correctIndex: 1,
      explanation: "医療では「AIが異常と判定しました」だけでは不十分です。Grad-CAM等のXAI手法により、AIが画像のどの部分を見て判断したかを可視化でき、医師が「AIの根拠が妥当か」を確認できます。EIRLの動脈瘤候補マーキングなど、実際の医療AI製品にも応用されています。"
    }
  ]}
/>

<ActionItem>
次にChatGPTやClaudeに医学的な質問をした際、回答に含まれる具体的な数字（統計データ、薬剤用量、論文の引用）を1つ選び、一次資料で正確性を検証してみましょう。「ハルシネーションチェック」を習慣化することが、生成AIの安全な活用の第一歩です。
</ActionItem>
