---
title: "セキュリティとサイバーセキュリティ"
description: "医療AIのセキュリティ脅威、敵対的攻撃、サイバーセキュリティのベストプラクティスと規制要件を学びます"
order: 8
estimatedMinutes: 22
---

# セキュリティとサイバーセキュリティ

## 1. はじめに

医療AIは、患者の機密性の高い健康データを扱います。このデータが漏洩、改ざん、または破壊されれば、患者のプライバシーが侵害されるだけでなく、誤診や不適切な治療につながる可能性があります。さらに、AIシステムそのものがサイバー攻撃の標的となり、悪意のある操作によって誤った判断を下すリスクもあります。

## 2. 医療AIのセキュリティ脅威

- **データ漏洩:** 患者の健康データが不正にアクセスされ、外部に漏洩する
- **データの改ざん:** AIの学習データや入力データが悪意を持って改ざんされる
- **モデルの盗難:** AIモデルそのものが盗まれ、不適切に利用される
- **サービス拒否攻撃 (DoS):** AIシステムに大量のリクエストを送り、システムを停止させる
- **内部脅威:** 医療機関の内部者がデータやシステムに不正にアクセスする
- **サプライチェーン攻撃:** 第三者ベンダー経由でAIシステムが侵害される

## 3. サイバーセキュリティのベストプラクティス

### 3.1 データの暗号化

保存時の暗号化（Encryption at Rest）と転送時の暗号化（Encryption in Transit / TLS/SSL）を実装します。

### 3.2 アクセス制御

役割ベースのアクセス制御（RBAC）、多要素認証（MFA）、最小権限の原則を実装します。

### 3.3 監査ログとモニタリング

すべてのアクセスと操作をログに記録し、リアルタイムで異常なパターンを検出します。

### 3.4 脆弱性管理

脆弱性スキャン、ペネトレーションテスト、パッチ管理を定期的に実施します。

### 3.5 セキュリティトレーニング

フィッシング攻撃の識別、強力なパスワード管理、データの取り扱いとプライバシー保護、インシデントの報告手順について教育します。

## 4. AIに特有のセキュリティ課題

### 4.1 Adversarial Attacks（敵対的攻撃）

AIの入力データに微小な変更を加えることで、AIを騙して誤った判断を下させる攻撃。対策として、Adversarial Training、入力の検証、モデルのアンサンブルがあります。

### 4.2 Model Inversion Attacks（モデル反転攻撃）

AIモデルから、学習データの情報を逆算して抽出する攻撃。差分プライバシーとモデルへのアクセス制限が対策となります。

### 4.3 Data Poisoning（データ汚染）

AIの学習データに悪意のあるデータを混入させる攻撃。データの検証、異常検出、信頼できるデータソースの使用が対策です。

### 4.4 Model Extraction（モデル抽出）

AIモデルにクエリを送り、モデルの構造やパラメータを推測・複製する攻撃。クエリ制限と応答のランダム化が対策です。

## 5. 規制要件

- **HIPAA:** セキュリティルール（アクセス制御、暗号化、監査ログ、インシデント対応）、違反時最大年間150万ドルの罰金
- **GDPR:** データ最小化、セキュリティ措置、データ侵害の72時間以内通知、違反時最大年間売上高の4%の罰金
- **FDA Cybersecurity Guidance:** 市販前のSecurity by Design、市販後の脆弱性監視とパッチ提供

## 6. 実例

- **WannaCry (2017):** 英国NHSの約80病院が影響、パッチ管理の重要性を実証
- **Anthem (2015):** 7,900万人の個人情報漏洩、暗号化の重要性を実証
- **医療画像への敵対的攻撃（研究）:** AIの頑健性向上の必要性を示した

## 7. 将来の展望

- **ゼロトラストアーキテクチャ:** 「信頼しない、常に検証する」原則のセキュリティモデル
- **ブロックチェーン:** データの改ざん防止と透明性の確保
- **量子暗号:** 量子コンピュータ時代に対応した強固な暗号技術

## 8. まとめ

医療AIのセキュリティとサイバーセキュリティは、患者の安全とプライバシーを守るために不可欠です。多様な脅威に対して多層防御を講じ、規制要件を遵守し、セキュリティを組織文化に組み込むことが重要です。

次のレッスンでは、医療AIの社会的影響と未来について学びます。
