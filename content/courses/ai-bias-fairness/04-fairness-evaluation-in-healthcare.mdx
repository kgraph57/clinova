---
title: "医療での公平性の評価"
description: "eGFR計算式の人種補正廃止、COMPAS再犯予測との比較から、医療特有の公平性評価の課題と方法を学びます"
order: 4
estimatedMinutes: 22
---

# 医療での公平性の評価

## はじめに — eGFR計算式から「人種」が消えた日

長年にわたり、腎機能の指標であるeGFR（推算糸球体濾過量）の計算式には「人種補正係数」が含まれていました。黒人患者には1.159の補正係数が乗じられ、eGFRが**高めに算出**されていたのです。

2021年、NKF（全米腎臓財団）とASN（米国腎臓学会）は、この人種補正を**廃止**する新しいCKD-EPI式を推奨しました。人種補正の結果、黒人患者の腎疾患の重症度が過小評価され、透析や腎移植の紹介が遅れるという**実害**が生じていたためです。

この事例は、「人種」という属性をAI/アルゴリズムでどう扱うかが、患者の生死に直結する問題であることを示しています。

<ResourceCard
  type="paper"
  title="New Creatinine- and Cystatin C-Based Equations to Estimate GFR without Race"
  url="https://www.nejm.org/doi/full/10.1056/NEJMoa2102953"
  source="New England Journal of Medicine"
  year={2021}
  description="人種補正を廃止した新しいCKD-EPI式。黒人患者の腎疾患診断の公平性を改善"
/>

---

## 医療における「保護される属性」

### 一般的な保護属性

| 属性 | 公平性の観点 | 医療での特殊性 |
|:---|:---|:---|
| 人種・民族 | 歴史的差別からの保護 | 一部の疾患リスクに生物学的な差異がある場合も |
| 性別 | ジェンダーバイアスの排除 | 性別特有の疾患が存在する |
| 年齢 | 年齢差別の防止 | 年齢は多くの疾患の重要なリスク因子 |
| 社会経済的地位 | 格差の再生産の防止 | 医療アクセスの格差がデータに反映 |
| 地理的位置 | 地域格差の防止 | 都市部と地方の医療資源の差 |

<Callout type="comparison" title="「臨床的に正当な区別」と「差別」の境界線">

**臨床的に正当な区別の例**: 女性のみに乳がんスクリーニングを推奨する。これは生物学的な差異に基づく臨床的に正当な区別。

**差別の例**: 黒人患者の痛みの訴えをAIが系統的に低く評価する。これは歴史的バイアスの再生産。

→ 属性の使用が「臨床的に正当か」「歴史的バイアスの再生産か」を判断するには、**臨床専門家と倫理学者の協力**が不可欠。

</Callout>

---

## 医療特有の公平性評価の課題

### 有病率の違いとその影響

疾患の有病率はグループ間で異なることが多く、これが公平性評価を複雑にします。

**例**: 2型糖尿病の有病率は、米国で白人（7.5%）に比べて黒人（11.7%）やヒスパニック（12.5%）で高い。この場合:

- **人口統計学的パリティ**（全グループで同じ陽性率）を適用すると、有病率の高いグループで見逃しが増える
- **等化オッズ**（全グループで同じ感度・偽陽性率）の方が臨床的に適切

### 代理指標の問題

Optumアルゴリズムの事例で見たように、「医療費」のような代理指標は社会構造の不平等を反映している場合があります。

<CaseStudy title="COMPAS再犯予測 vs 医療AI — 公平性論争の教訓" year={2016} jurisdiction="米国" tags={["COMPAS", "公平性論争", "比較研究"]}>

**背景**: 2016年、ProPublicaは刑事司法で使われる再犯予測アルゴリズムCOMPASが、黒人被告を「高リスク」と過剰に判定していると報告。

**論争**: ProPublicaは「偽陽性率が黒人で高い（等化オッズ違反）」と指摘。Northpointe社は「予測値パリティは達成している」と反論。**両者とも正しかった**が、異なる公平性指標を使っていた。

**医療AIへの教訓**:
- どの公平性指標を使うかで結論が正反対になりうる
- 医療では「見逃し」（偽陰性）が患者の生命に直結するため、**等化オッズ**が特に重視される
- 公平性指標の選択は技術的判断ではなく、**価値判断**

</CaseStudy>

---

## 実践的な公平性評価の4ステップ

### ステップ1: 評価対象の定義

**何を評価するか**: どの属性（人種、性別、年齢等）でグループ分けするか
**いつ評価するか**: 開発時、検証時、そして**運用開始後も継続的に**

### ステップ2: サブグループ別の性能評価

各サブグループで以下を計算:
- 感度（真陽性率）: 実際に疾患がある患者を正しく検出する率
- 特異度（真陰性率）: 健康な患者を正しく除外する率
- AUC: 全体的な判別能力
- PPV / NPV: 陽性・陰性判定の信頼性

### ステップ3: 公平性指標の計算と比較

複数の公平性指標を計算し、どの指標で不公平が生じているかを特定:

| 指標 | 比較対象 | 問題あり判定の基準（目安） |
|:---|:---|:---|
| 等化オッズ | グループ間の感度差・偽陽性率差 | 差が5ポイント以上 |
| 予測値パリティ | グループ間のPPV差 | 差が5ポイント以上 |
| 人口統計学的パリティ | グループ間の陽性率差 | 差が有病率の差で説明できない |

### ステップ4: 臨床的文脈での解釈

統計的な差だけでなく、**臨床的にその差が患者にどのような影響を与えるか**を評価:

- 感度が8ポイント低いグループでは、がんの見逃し率がどれだけ増えるか？
- 偽陽性率が高いグループでは、不必要な生検がどれだけ増えるか？
- これらの影響は、患者の転帰にどの程度影響するか？

<Callout type="question" title="考えてみよう">

ある肺がんスクリーニングAIで以下の結果が得られました:

- 男性の感度: 94%、偽陽性率: 8%
- 女性の感度: 88%、偽陽性率: 12%

等化オッズの観点では、感度に6ポイント、偽陽性率に4ポイントの差があります。年間10万人のスクリーニングで、この差はどれだけの「見逃し」と「不必要な追加検査」を生むでしょうか？

このような**臨床的影響の定量化**が、統計的な公平性指標を実際の医療改善につなげる鍵です。

</Callout>

---

## まとめ

医療における公平性評価は、統計的な指標の計算にとどまりません。eGFRの人種補正廃止の事例が示すように、アルゴリズムに組み込まれた属性の扱い方が患者の転帰を左右します。有病率の違い、代理指標の問題、「臨床的に正当な区別」と「差別」の境界線など、医療特有の課題を理解した上で公平性を評価する必要があります。

次のレッスンでは、公平性を確保するための**ベストプラクティス**を学びます。

<Quiz
  courseId="ai-bias-fairness"
  lessonSlug="04-fairness-evaluation-in-healthcare"
  questions={[
    {
      question: "eGFR計算式から人種補正が廃止された理由はどれか？",
      options: ["計算が複雑になりすぎたため", "人種補正により黒人患者の腎疾患の重症度が過小評価され、治療の紹介が遅れていたため", "人種データの収集が困難になったため", "新しい検査機器が導入されたため"],
      correctIndex: 1,
      explanation: "人種補正により黒人患者のeGFRが高めに算出され、腎疾患の重症度が過小評価されていました。これにより透析や腎移植の紹介が遅れるという実害が生じていたため、2021年にNKFとASNが人種補正の廃止を推奨しました。"
    },
    {
      question: "COMPAS再犯予測の公平性論争が医療AIに与える教訓はどれか？",
      options: ["AIは刑事司法にのみバイアスの問題がある", "公平性指標の選択によって結論が正反対になりうるため、指標の選択は価値判断である", "すべての公平性指標を同時に満たすモデルを作ればよい", "医療AIにはCOMPASの知見は適用できない"],
      correctIndex: 1,
      explanation: "COMPASの論争では、ProPublicaとNorthpointe社が異なる公平性指標を使い、両者とも正しいが異なる結論に達しました。どの指標を優先するかは技術的判断ではなく価値判断であり、医療では「見逃し」の影響から等化オッズが重視されます。"
    },
    {
      question: "医療の公平性評価で「人口統計学的パリティ」が不適切になる場合はどれか？",
      options: ["データ量が少ない場合", "疾患の有病率がグループ間で異なる場合", "AIモデルが深層学習を使用している場合", "保護属性が3つ以上ある場合"],
      correctIndex: 1,
      explanation: "疾患の有病率がグループ間で異なる場合、同じ陽性率を目指す人口統計学的パリティは臨床的に不適切です。有病率の高いグループで見逃しが増えたり、有病率の低いグループで過剰診断が増えたりします。"
    }
  ]}
/>

<ActionItem>
自施設で導入済みまたは導入検討中のAIシステムについて、評価対象とすべき「保護される属性」のリストを作成してみましょう。その属性ごとにサブグループ別の性能を確認し、「臨床的に正当な差異」と「是正すべきバイアス」を区別する議論をチームで始めましょう。
</ActionItem>
