---
title: "転移学習の活用"
description: "限られた医療画像データから高性能モデルを構築する転移学習—Feature ExtractionとFine-tuningの使い分け、事前学習モデルの選定、実践的なベストプラクティスを学びます"
order: 4
estimatedMinutes: 18
---

## 転移学習とは何か

転移学習（Transfer Learning）とは、あるタスク・ドメインで学習した知識を別のタスクに転用する手法です。医療画像AIでは、ImageNetのような大規模自然画像データセットで事前学習されたモデルを出発点とし、比較的少量の医療画像データで追加学習を行います。

### なぜ医療画像AIで転移学習が不可欠なのか

医療画像AI開発には3つの構造的障壁があります。

- **ラベル付きデータの不足** — 医療画像のアノテーションには専門医の関与が必要で、1枚あたりのコストが高い
- **アノテーションの品質管理** — 読影の一致率（inter-reader agreement）は疾患や経験年数により変動する
- **ゼロから学習するコスト** — 数百万パラメータのCNNをスクラッチで学習するには膨大なデータと計算時間が必要

転移学習はこれらの障壁を大幅に引き下げ、数百〜数千枚規模のデータセットでも実用的な精度のモデル構築を可能にします。

<Callout type="insight" title="ImageNetの特徴は医療画像にも通用する">
一見すると自然画像と医療画像は異質ですが、CNNの浅い層が学習するエッジ・テクスチャ・コントラストなどの低レベル特徴はドメインを問わず共通です。深い層の高次特徴のみを医療画像で再学習する戦略が有効です。
</Callout>

---

## 2つの主要アプローチ

### Feature Extraction（特徴抽出）

事前学習モデルの畳み込み層を特徴抽出器として固定し、分類層のみを新たに学習する手法です。

**手順:**
1. 事前学習モデル（ResNet、EfficientNetなど）をロード
2. 最終の全結合層（分類ヘッド）を削除
3. 畳み込み層のパラメータを凍結（学習不可に設定）
4. 新しい分類層を追加（出力ノード数をタスクに合わせて設定）
5. 分類層のパラメータのみを学習

**適するケース:**
- データが非常に少ない（数百枚規模）
- 計算リソースが限られている
- ターゲットドメインがソースドメインに比較的近い

### Fine-tuning（微調整）

事前学習モデルの重みを初期値として、ネットワーク全体を再学習する手法です。

**手順:**
1. 事前学習モデルをロード
2. 分類ヘッドをタスクに合わせて置換
3. 浅い層には低い学習率を設定（例: 1e-5）
4. 深い層には高い学習率を設定（例: 1e-3）
5. 全体を段階的に学習（Gradual Unfreezing戦略）

**適するケース:**
- 一定量のデータがある（数千枚以上）
- ターゲットドメインがソースドメインと大きく異なる
- 最高精度を追求したい

<Callout type="comparison" title="Feature Extraction vs Fine-tuning">
データが少ない初期段階ではFeature Extractionで素早くベースラインを構築し、データが蓄積されたらFine-tuningに移行するのが実践的な戦略です。Feature Extractionは数分〜数十分、Fine-tuningは数時間〜数日の学習時間がかかる場合があります。
</Callout>

---

## 主要な事前学習モデル

### ImageNet事前学習モデル

ImageNetは1,400万枚以上の画像、1,000クラスを含む大規模画像認識データセットで、以下のモデルが事前学習されています。

**VGG-16/19:**
- シンプルな3×3畳み込みの積み重ね
- 構造が理解しやすく教育目的に適する
- パラメータ数が多く（138M）、推論が重い

**ResNet-50/101/152:**
- 残差接続（Skip Connection）により深いネットワークの学習を安定化
- 医療画像AIで最も広く使われるベースモデルの一つ
- 50層〜152層のバリエーションがある

**EfficientNet-B0〜B7:**
- 幅・深さ・解像度を複合的にスケーリング
- パラメータ効率が高く、同等精度でモデルサイズが小さい
- 計算リソースが限られる医療現場に適する

**DenseNet-121/169:**
- 各層が前のすべての層と接続される密結合構造
- 特徴の再利用効率が高い
- 胸部X線解析（CheXNet）で採用実績がある

### 医療画像特化の事前学習

近年はImageNet以外にも、医療画像で事前学習されたモデルが登場しています。

- **Med3D** — 3D医療画像（CT/MRI）用の事前学習モデル
- **Models Genesis** — 自己教師あり学習で医療CTから事前学習
- **BiomedCLIP** — 医学文献の画像・テキストペアで学習したマルチモーダルモデル

<Callout type="insight" title="モデル選定の実務的指針">
迷ったらまずResNet-50でベースラインを構築しましょう。そのうえでEfficientNetやDenseNetを試し、精度と推論速度のトレードオフを確認します。3D入力が必要な場合はMed3Dの検討をおすすめします。
</Callout>

---

## 実践例

### 胸部X線の異常検出

**構成:**
1. ImageNetで事前学習したResNet-50をベースモデルとしてロード
2. 最終の全結合層を14クラス出力に置換（CheXpert準拠）
3. 胸部X線データ（10,000枚）でFine-tuning
4. 浅い層は学習率1e-5、深い層は1e-3で段階的にUnfreeze

**結果:**
- スクラッチ学習と比較してAUCが0.07向上
- 学習に必要なエポック数が1/3に短縮
- データ不足の希少疾患でも一定の性能を確保

### 眼底写真の糖尿病性網膜症スクリーニング

**構成:**
1. EfficientNet-B3をロード
2. Feature Extraction方式で分類層のみ学習（データ5,000枚）
3. 性能確認後、後半20層をUnfreezeしてFine-tuning

**結果:**
- Feature Extraction段階でAUC 0.88を達成
- Fine-tuning後にAUC 0.94まで向上
- 少量データでも段階的アプローチが有効であることを実証

---

## ベストプラクティス

### 1. データ量に応じてアプローチを切り替える

| データ量 | 推奨アプローチ |
|---|---|
| 100〜500枚 | Feature Extraction |
| 500〜5,000枚 | Gradual Unfreezing |
| 5,000枚以上 | Full Fine-tuning |

### 2. 学習率の差別化（Discriminative Learning Rates）

層ごとに異なる学習率を設定するのが標準的なプラクティスです。

- 浅い層（汎用的な特徴）: 1e-5程度
- 中間層: 1e-4程度
- 深い層（タスク固有の特徴）: 1e-3程度

### 3. データ拡張との併用

転移学習とデータ拡張は相互補完的です。特にデータが少ない状況では、両者の組み合わせが不可欠です。

### 4. 複数モデルの比較検証

単一のモデルに依存せず、最低2〜3種類の事前学習モデルを比較します。精度だけでなく推論時間・メモリ使用量も含めて総合的に判断しましょう。

<Callout type="warning" title="Fine-tuningの過学習リスク">
データが少ない状態でFine-tuningを行うと、モデルが訓練データに過適合し汎化性能が低下するリスクがあります。検証データの損失が上昇し始めたらEarly Stoppingを適用し、Feature Extractionへの切り替えも検討してください。
</Callout>

---

## まとめ

- 転移学習は医療画像AIの「データ不足」問題を解決する中核的アプローチ
- Feature ExtractionとFine-tuningをデータ量に応じて使い分ける
- ResNet、EfficientNet、DenseNetなどの事前学習モデルが利用可能
- 学習率の差別化、Early Stopping、データ拡張の併用が成功の鍵

次のレッスンでは、構築したモデルの性能を正しく測定するための評価指標と検証手法を学びます。

---

<Quiz courseId="medical-imaging-ai-basics" lessonSlug="transfer-learning" questions={[
  {
    question: "データが200枚しかない状況で、転移学習のアプローチとして最も適切なものはどれですか？",
    options: [
      "CNNをゼロからスクラッチで学習する",
      "事前学習モデルの畳み込み層を凍結し、分類層のみ学習する（Feature Extraction）",
      "事前学習モデル全体を高い学習率でFine-tuningする",
      "転移学習は使わずランダムフォレストで分類する"
    ],
    correctIndex: 1,
    explanation: "データが非常に少ない場合（数百枚規模）はFeature Extraction方式が適切です。畳み込み層を凍結して分類層のみ学習することで、過学習リスクを抑えながら事前学習の特徴量を活用できます。"
  },
  {
    question: "転移学習のFine-tuningで「Discriminative Learning Rates」を用いる理由として正しいものはどれですか？",
    options: [
      "すべての層を同じ速度で学習させるため",
      "浅い層の汎用的な特徴を保持しつつ、深い層をタスク固有に適応させるため",
      "学習時間を最大化するため",
      "GPU使用率を均一にするため"
    ],
    correctIndex: 1,
    explanation: "浅い層はエッジ・テクスチャなどの汎用特徴を学習しており、大きく更新すると有用な知識が失われます。深い層はタスク固有の高次特徴を学ぶため、高い学習率で積極的に更新する戦略が有効です。"
  }
]} />

<ActionItem>
PyTorchまたはTensorFlowを使って、ImageNet事前学習済みのResNet-50を読み込み、最終層を2クラス分類（正常/異常）に置換してみましょう。まずFeature Extraction方式で畳み込み層を凍結し、任意のデータセット（Kaggle Chest X-ray等）で分類層のみ学習させ、テスト精度を記録してください。その後、Gradual Unfreezing方式に切り替えて精度の変化を比較しましょう。
</ActionItem>
