---
title: "なぜ医療AIに倫理と法律が不可欠なのか — JAMA RCT事件からの教訓"
description: "医療AIの倫理的・法的リテラシーが欠如するとどうなるのか。JAMA RCT事件を起点に、本書の目的と構成を示す。"
slug: "00-00-preface"
order: 0
partId: "preface"
partTitle: "はじめに"
partOrder: 0
estimatedMinutes: 20
free: true
status: "published"
tags: ["医療AI", "倫理", "法律", "JAMA", "RCT", "インフォームドコンセント"]
publishedAt: "2026-03-01"
updatedAt: "2026-03-01"
---

**「AIを使えば医療は良くなる」という素朴な信念は、すでに崩れている。問題は、AIの能力ではなく、それを取り巻く倫理と法律の不在である。**

---

## JAMA RCT事件が突きつけたもの

2025年、*JAMA Internal Medicine* に掲載された無作為化比較試験（RCT）は、医療AI楽観論に冷水を浴びせた。50名の医師を対象に、AIを使った診断精度を検証した結果、**AIの支援を受けた医師群は、AIなしの医師群よりも診断精度が低かった**のである。

この結果は、「AIを導入すれば自動的に医療の質が向上する」という前提が誤りであることを示した。しかし、この研究が突きつける問題は、診断精度の話だけにとどまらない。

<Callout type="warning" title="JAMA RCT事件の倫理・法律的含意">
この研究が明らかにしたのは、AI支援による診断精度の低下だけではない。訓練なしにAIを使う医師が患者に害を与えうるという事実は、インフォームドコンセント、医師の注意義務、製造物責任など、複数の倫理的・法的問題を同時に提起している。
</Callout>

---

## 倫理と法律の不在がもたらす5つのリスク

医療AIを倫理的・法的枠組みなしに導入すると、以下のリスクが現実化する。

### リスク1：患者への直接的被害

AIの誤診断や誤判断が患者に直接的な害をもたらすリスクである。2023年、米国で放射線AIが乳がんの偽陰性を出力し、患者の診断が6か月遅れた事例が報告されている。訓練されていない医師がAI出力を無批判に受け入れた結果であった。

### リスク2：法的責任の曖昧化

AIが関与した医療行為で過誤が生じた場合、責任の所在が不明確になる。医師の過失なのか、AI開発者の製造物責任なのか、病院の管理責任なのか。日本の現行法は、この問いに明確な回答を持っていない。

### リスク3：プライバシー侵害

ChatGPTやClaudeなどの汎用AIに患者情報を入力する医療者は少なくない。2023年のメディカルAI学会の調査では、回答した医師の**23%**が「患者情報をAIに入力したことがある」と答えている。これは個人情報保護法第27条（第三者提供の制限）に抵触する可能性がある。

### リスク4：医療格差の拡大

AIのアルゴリズムバイアスにより、特定の人種、性別、年齢層の患者が不利な診断や治療推奨を受けるリスクがある。米国のOptum社の医療AIが、黒人患者に対して白人患者より低いリスクスコアを付与していた事例（2019年、*Science* 誌）は、この問題の深刻さを示している。

### リスク5：信頼の崩壊

倫理的・法的裏付けのないAI活用は、患者-医師間の信頼関係を損なう。「先生はAIに判断を丸投げしているのではないか」という不信感は、一度生じると回復が極めて困難である。

<StatHighlight value="23%" label="患者情報をAIに入力した経験がある医師の割合" source="メディカルAI学会調査, 2023" />

---

## 「技術中心主義」から「人間中心主義」へ

医療AIの議論は、長らく「何ができるか」（技術的可能性）に偏重してきた。しかし本来問うべきは以下の3つである。

1. **何をすべきか**（倫理的正当性）
2. **何が許されるか**（法的適法性）
3. **何を守るべきか**（患者の権利と尊厳）

この3つの問いに体系的に答えることが、本書の目的である。

<ComparisonCard
  title="医療AI導入の2つのアプローチ"
  left={{
    title: "技術中心主義",
    items: [
      "何ができるか？",
      "精度が高ければよい",
      "導入速度を優先",
      "結果として問題が後から噴出"
    ]
  }}
  right={{
    title: "人間中心主義（本書のアプローチ）",
    items: [
      "何をすべきか？",
      "精度＋倫理＋法的適合",
      "安全性と信頼を優先",
      "持続可能な導入を実現"
    ]
  }}
/>

---

## 本書の構成

本書は8つのPartで構成される。

### Part 1：医療倫理の基盤
ビーチャム＆チルドレスの医療倫理4原則（自律尊重・善行・無危害・正義）をAI時代に読み替える。インフォームドコンセントと患者の自律性について、AIが生み出す新たな課題を検討する。

### Part 2：AI固有の倫理課題
アルゴリズムバイアス、ブラックボックス問題（説明可能AI）、AIの誤判断における責任の所在、ハルシネーションの倫理的リスクなど、従来の医療倫理では想定されていなかった課題を扱う。

### Part 3：日本の法制度
医師法・医療法とAIの関係、個人情報保護法（改正法2022）、次世代医療基盤法、薬機法・PMDAによるSaMD規制を、条文レベルで解説する。

### Part 4：世界の規制動向
EU AI Act、FDA（米国食品医薬品局）の規制アプローチ、WHO・UNESCOのグローバルガバナンスを概観する。

### Part 5：患者データとプライバシー
最小限データの原則、クラウドAIサービスへのデータ入力リスク、データ二次利用の同意と透明性を実務レベルで解説する。

### Part 6：組織ガバナンス
病院AI倫理委員会の設計、院内AI利用ポリシーのテンプレート、AIインシデント対応プロトコルを提供する。

### Part 7：未来の倫理・法律
自律型AI時代の倫理フレームワークと、日本の医療AI規制の展望・提言を述べる。

---

## 本書の読者

本書は以下の読者を想定している。

- **臨床医**：日常診療でAIを使い始めた、または使おうとしている医師
- **病院管理者**：AI導入の意思決定を行う経営層・管理部門
- **医療情報部門**：AI導入の技術的・制度的対応を担う部門
- **医療AI開発者**：医療分野のAIプロダクトを開発するエンジニア・研究者
- **医学生・研修医**：AI時代の医師としてのキャリアを考える若手医療者
- **医療政策立案者**：医療AI規制の設計に関わる行政関係者

---

## 法律と倫理は「制約」ではなく「基盤」である

最後に、本書の根底にある信念を述べておきたい。

法律と倫理を「面倒な制約」と捉える向きがある。「規制が多いとイノベーションが阻害される」という主張は、テック業界では広く流通している。

しかし、医療においてこの考えは危険である。

**医療における法律と倫理は、患者の命と尊厳を守るための「基盤」であり、「制約」ではない。**

飛行機にとっての航空法は、飛行を制約するものではなく、安全に飛ぶための基盤である。同様に、医療AIにとっての倫理と法律は、AIの活用を制約するものではなく、**安全に活用するための基盤**である。

この基盤を持たないAI活用は、基礎なき建築と同じである。一時的には立つかもしれないが、いずれ崩壊する。

<Warning>
本書の内容は2026年3月時点の法令・規制に基づいています。医療AI規制は急速に変化しており、最新の法改正・ガイドライン改訂については、厚生労働省、個人情報保護委員会、PMDAの公式サイトを必ずご確認ください。本書は法的助言を構成するものではなく、個別の事案については弁護士に相談することを推奨します。
</Warning>

<KeyTakeaway>
- JAMA RCT事件は、AIの「技術的性能」だけでなく「倫理的・法的枠組み」の必要性を実証した
- 倫理・法律の不在がもたらすリスクは、患者被害、法的責任の曖昧化、プライバシー侵害、医療格差拡大、信頼崩壊の5つに整理できる
- 医療AIの議論は「何ができるか」（技術中心主義）から「何をすべきか」（人間中心主義）へ転換すべきである
- 法律と倫理は医療AIの「制約」ではなく「基盤」であり、安全な活用のための前提条件である
- 本書は8つのPartで、倫理の基盤からAI固有の課題、日本の法制度、世界の規制、プライバシー、組織ガバナンス、未来の展望までを体系的にカバーする
</KeyTakeaway>
