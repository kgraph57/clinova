---
title: "ハルシネーションの倫理的リスク"
description: "LLMのハルシネーション（幻覚）が医療において引き起こす倫理的リスクと、その管理策を解説する。"
slug: "02-04-hallucination-ethics"
order: 7
partId: "part02"
partTitle: "AI固有の倫理課題"
partOrder: 2
estimatedMinutes: 18
free: true
status: "published"
tags: ["ハルシネーション", "幻覚", "LLM", "誤情報", "患者安全", "ファクトチェック"]
publishedAt: "2026-03-01"
updatedAt: "2026-03-01"
---

**LLMはもっともらしい嘘をつく。医療においてこの「もっともらしさ」こそが最大の危険である。なぜなら、嘘が見破れないほど巧みであれば、誤った情報に基づく医療行為が行われてしまうからだ。**

---

## ハルシネーションとは何か

ハルシネーション（hallucination）とは、大規模言語モデル（LLM）が事実に基づかない情報を、あたかも事実であるかのように生成する現象を指す。

### ハルシネーションの種類

**事実的ハルシネーション**（Factual Hallucination）：
存在しない事実の生成。例：「メトホルミンは1968年にFDAで承認された」（実際は1994年）。

**引用ハルシネーション**（Citation Hallucination）：
存在しない論文の引用。例：「Smith et al. (2023) がNEJMに発表した研究では...」（そのような論文は存在しない）。

**推論ハルシネーション**（Reasoning Hallucination）：
正しい前提から誤った結論を導く。例：「この患者のBMIは25であり、肥満に該当するため...」（BMI 25は過体重であり肥満ではない）。

**文脈ハルシネーション**（Contextual Hallucination）：
与えられた文脈（患者情報など）に基づかない情報の生成。例：患者の投薬リストに含まれていない薬剤の相互作用を指摘する。

<StatHighlight value="5-27%" label="LLMの医療関連回答におけるハルシネーション率" source="各種ベンチマーク研究の範囲, 2024-2025" />

---

## 医療におけるハルシネーションの倫理的問題

### 問題1：無危害原則への違反

ハルシネーションに基づく医療判断は、患者に直接的な害をもたらしうる。

**事例**：LLMが「この患者の症状パターンからはスティーブンス・ジョンソン症候群の可能性を検討すべき」と出力したが、その根拠としたバイタルサインの値がハルシネーションであった場合、不必要な入院、検査、患者の不安を招く。

### 問題2：信頼の毀損

医師がLLMのハルシネーションに気づかず、誤った情報を患者に伝えた場合、患者-医師間の信頼関係が根本的に損なわれる。一度でも「AIが嘘をついたのに先生はそのまま伝えた」という経験をした患者は、以後の医療全般に対する不信感を抱く。

### 問題3：インフォームドコンセントの無効化

ハルシネーションに基づいて患者に説明が行われた場合、その説明に基づく同意は「十分な情報」に基づいていないため、インフォームドコンセントの要件を満たさない。

### 問題4：医療資源の浪費（正義の原則）

ハルシネーションに基づく不必要な検査や治療は、限りある医療資源を浪費する。これは正義の原則（公正な資源配分）に反する。

### 問題5：法的責任の複雑化

ハルシネーションに起因する医療過誤において、責任の所在が複雑化する。医師はハルシネーションの存在を知りながらLLMを使用した（または知るべきであった）場合、過失が認定されうる。

<Callout type="warning" title="「もっともらしさ」の罠">
ハルシネーションが危険である最大の理由は、その「もっともらしさ」にある。LLMは文法的に正確で、医学用語を適切に使い、論理的な構造を持つ文章を生成する。このため、内容が事実に反していても、一見すると見破ることが困難である。特に、専門外の領域や夜勤中の疲労した状態では、検証が不十分になるリスクが高い。
</Callout>

---

## ハルシネーションが起きやすい医療場面

### 1. 稀少疾患の鑑別

LLMの学習データにおける稀少疾患の情報は限られており、一般的な疾患の特徴を稀少疾患に不適切に当てはめる傾向がある。

### 2. 薬剤情報

薬剤の用量、相互作用、禁忌に関する情報は、正確性が生命に直結する。LLMはしばしば用量を誤り、存在しない相互作用を報告する。

### 3. ガイドラインの引用

最新のガイドラインの内容を正確に引用することはLLMの苦手とするところであり、古いガイドラインの内容を最新として提示したり、ガイドラインの推奨度を誤って報告したりする。

### 4. 文献の引用

PubMedの論文を引用する際、存在しない論文（著者名、雑誌名、年号の組み合わせが架空）を生成する頻度が高い。

### 5. 法的情報

本書の主題に直結する問題として、LLMは法律の条文番号、判例の日付、規制の内容を誤って生成することがある。

---

## ハルシネーション管理の倫理的フレームワーク

### 原則1：ハルシネーションの存在を前提とする

LLMを医療に使用する場合、ハルシネーションが発生する前提で運用する。「ハルシネーションが起きないことを期待する」のではなく、「起きたときに発見・対処できる仕組み」を構築する。

### 原則2：検証可能性の確保

LLMの出力は、常に独立した情報源で検証可能でなければならない。検証できない出力は、意思決定に使用してはならない。

### 原則3：ハイリスク場面での使用制限

生命に直結する判断（薬剤の用量決定、手術適応の判定など）では、LLMの出力を直接的な根拠として使用しない。

### 原則4：透明性

LLMの出力にハルシネーションの可能性があることを、医師自身が認識し、患者にも適切に伝える。

---

## 実践的なハルシネーション対策

<StepFlow>
<StepFlowStep number={1} title="出力の構造化">
LLMに「出典を明記して回答してください」と指示する。引用された出典が実在するかを確認することで、ハルシネーションを検出しやすくなる。
</StepFlowStep>
<StepFlowStep number={2} title="クロスバリデーション">
重要な情報は、LLM以外の情報源（UpToDate、PubMed、添付文書、ガイドラインの原本）で必ず確認する。
</StepFlowStep>
<StepFlowStep number={3} title="用途の限定">
LLMを「アイデア生成」「文章の整理」「一般的な情報の概観」に使用し、「事実の確認」「用量の決定」「法的判断」には使用しない。
</StepFlowStep>
<StepFlowStep number={4} title="複数モデルの利用">
重要な事項について複数のLLMに同じ質問をし、回答の一致を確認する。不一致がある場合は、信頼できる一次情報源で検証する。
</StepFlowStep>
<StepFlowStep number={5} title="RAG（検索拡張生成）の活用">
院内の信頼できるデータベース（電子カルテ、処方データ、ガイドライン）をLLMに接続するRAGシステムを活用し、ハルシネーションのリスクを低減する。
</StepFlowStep>
</StepFlow>

---

## ハルシネーションと法的責任

### 医師の注意義務

医師がLLMのハルシネーションを知りうべき状態でそれを看過した場合、注意義務違反が認定されうる。

**論点**：「標準的な医師」はLLMのハルシネーションリスクを認識しているべきか？

2026年時点において、LLMのハルシネーションは医学界で広く知られており、多くの学術論文や医学雑誌で注意喚起がなされている。したがって、「ハルシネーションの存在を知らなかった」は抗弁として成立しにくくなっている。

### AI開発者の表示義務

LLMの開発者は、ハルシネーションのリスクを利用者に適切に告知する義務がある。「この出力は必ずしも正確ではありません」といった一般的な免責事項だけでは不十分であり、医療用途に関しては具体的なリスクと限界の説明が求められる。

### 組織的責任

病院がLLMの医療利用を認めている場合、ハルシネーション対策のガイドラインを策定し、研修を実施する組織的責任がある。

---

## ハルシネーション検出技術の最前線

### 自己整合性チェック（Self-Consistency）

同じ質問を複数回LLMに投げ、回答の一致度を確認する手法。ハルシネーションは回答ごとに内容が変わりやすい。

### 不確実性推定（Uncertainty Estimation）

LLMの出力に対する「自信度」を推定する手法。自信度が低い出力はハルシネーションの可能性が高い。

### 外部知識ベースとの照合

LLMの出力を、構造化された医療知識ベース（UMLS、MeSH、添付文書データベースなど）と自動的に照合する手法。

### GPTZero等のハルシネーション検出ツール

2026年のICLR（International Conference on Learning Representations）では、GPTZeroチームがハルシネーション検出の新手法を発表し、医療テキストにおける検出精度の向上を報告している。

<ComparisonCard
  title="ハルシネーション対策のアプローチ"
  left={{
    title: "人的対策",
    items: [
      "出典の手動確認",
      "クロスバリデーション",
      "専門領域での知識に基づく検証",
      "複数医師によるレビュー"
    ]
  }}
  right={{
    title: "技術的対策",
    items: [
      "RAG（検索拡張生成）",
      "自己整合性チェック",
      "不確実性推定",
      "外部知識ベース照合"
    ]
  }}
/>

---

## 患者への説明

LLMの出力を参考にした医療判断を行う場合、患者に対して以下の説明を行うことが倫理的に求められる。

1. AIを参考情報として利用していること
2. AIには誤りが含まれる可能性があること
3. AIの情報は独立して検証していること
4. 最終判断は医師が行っていること

<PromptTemplate title="ハルシネーション対策の院内ルール（雛形）">
**目的**：LLMのハルシネーション（事実に基づかない出力）による患者被害を防止する

**1. 使用可能な場面**
- 一般的な医学知識の概観把握
- 文章の構成・推敲の支援
- 鑑別診断のブレインストーミング（要検証）

**2. 使用不可の場面**
- 薬剤用量の決定（必ず添付文書で確認）
- 法的判断（必ず弁護士に確認）
- 患者への説明文書の直接生成（必ず医師が確認・修正）

**3. 必須の検証プロセス**
- LLMが引用した文献は、PubMed等で存在を確認する
- 薬剤情報はPMDA添付文書データベースで確認する
- ガイドラインの記述は、原本で確認する
- 用量・禁忌は、薬剤部と確認する

**4. 記録**
- LLMの出力を参考にした旨をカルテに記載する
- 検証プロセスの結果を記録する
</PromptTemplate>

<KeyTakeaway>
- ハルシネーションは事実的、引用、推論、文脈の4種類に分類でき、医療では各種類が深刻なリスクとなる
- 倫理的には無危害原則への違反、信頼の毀損、インフォームドコンセントの無効化、医療資源の浪費に該当する
- 稀少疾患、薬剤情報、ガイドライン引用、文献引用、法的情報がハルシネーションの高リスク領域
- 管理の基本原則は「存在を前提とする」「検証可能性の確保」「ハイリスク場面の使用制限」「透明性」
- 法的には、ハルシネーションリスクの認識は医師の注意義務に含まれつつあり、「知らなかった」は抗弁として成立しにくい
- RAG、自己整合性チェック、不確実性推定などの技術的対策と、人的検証の組み合わせが実効的
</KeyTakeaway>
