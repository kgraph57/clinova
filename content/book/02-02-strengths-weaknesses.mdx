---
title: "医師が知るべきAIの得意と苦手 — 何を任せ、何を任せないか"
description: "AIの正しい使い方は「人間の代替」ではなく「人間の増幅」である。"
slug: "02-02-strengths-weaknesses"
order: 5
partId: "part02"
partTitle: "AIを理解する"
partOrder: 2
free: false
status: "published"
tags: ["臨床推論", "MCQ", "SCT", "診断精度", "検証コスト"]
estimatedMinutes: 13
publishedAt: "2026-03-01"
updatedAt: "2026-03-01"
---

**AIの正しい使い方は「人間の代替」ではなく「人間の増幅」である。**

---

## 92% 対 医学生以下 — 同じAIの二つの顔

「AIが医師国家試験に合格」——あなたもこのニュース見出しを見たことがあるだろう。

多肢選択式問題（MCQ）で92%の正答率。米国医師国家試験相当の問題をクリアしたAIが、いよいよ臨床医を不要にするのではないか。そんな論調がメディアを賑わせた。

しかし、同じ時期にもう一つの研究結果が発表されていた。

**不確実性下の臨床推論を測定するScript Concordance Test（SCT）750問において、同じAIが医学生にすら劣る成績を示した**のである。

MCQでは「天才」、SCTでは「初学者以下」。

同じモデルが、同じ医学知識の領域で、なぜこれほど対照的な結果を見せるのか。

この矛盾を解き明かすことが、**AIを正しく使うための第一歩**である。

---

## 圧倒的に強い領域 — 定型文書・検索・変換

まずは、AIが人間を明確に上回る領域から見ていこう。

共通するのは、**「大量の情報を一定のパターンに従って処理する」というタスクの性質**である。

### 1. カルテ記載と文書生成

診療中の会話をリアルタイムで記録し、構造化されたカルテに変換する「Ambient AI」の効果は、すでに無作為化比較試験（RCT）で実証されている。

**NEJM AIに掲載された研究によれば：**
- カルテ1件あたりの記載時間が平均**41秒短縮**
- 医師のバーンアウトスコアが**7%改善**

41秒という数字は一見小さく思えるかもしれない。しかし：

- 1日に30人の患者を診る医師であれば、1日あたり約**20分**
- 1カ月で**10時間**
- 1年で**120時間**の時間が生まれる計算

文書の品質についても、**BMJ Openに発表された研究がAI生成の医療文書と医師が作成した文書を比較し、品質に統計的な有意差がないことを示している。**

つまり、AIは医師と同等の品質の文書を、はるかに短い時間で作成できるのである。

### 2. 文献検索

医学文献の検索においても、AIの優位性は明確である。

**Journal of Clinical Epidemiologyに掲載された研究：**
- AI支援による文献検索が従来の手法と比較して所要時間を**67%短縮**
- 検索の精度（適合率・再現率）は維持

従来3時間かかっていた文献検索が1時間で完了する。しかも、見落としの率は変わらない。

研究に費やせる時間が限られている臨床医にとって、これは文献検索のハードルを劇的に下げる変化である。

### 3. 翻訳

多言語間の医療テキスト翻訳においても、AIは高い性能を示している。

機械翻訳の品質指標であるBLEUスコアで**0.78**を記録しており、これは「非常に良好な翻訳品質」と評価される水準である。

英語論文の要旨を日本語で理解したい、あるいは海外からの患者家族に日本語の検査結果を説明したいといった場面で、AIは実用的な翻訳を即座に提供できる。

---

## 決定的に弱い領域 — 不確実性下の推論

続いて、AIが人間に明確に劣る領域を見ていく。

### MCQで92%、SCTで人間以下 — 同じAIの二つの顔

2025年、NEJM AIに発表された研究が、この問題を定量的に示した。

**最先端のLLMが：**
- 多肢選択式問題（MCQ）で**86～92%**の正答率を達成
- Script Concordance Test（SCT）750問では**医学生の平均スコアにも届かなかった**

### MCQとSCTの違い

**MCQ**は基本的に「正解が一つ存在する」問題であり、知識の再現が問われる。

**SCT**は「この追加情報が得られたとき、あなたの仮説はどの程度変化するか」を問う形式であり、**不確実性のもとでの推論——すなわち臨床推論の核心——が試される。**

AIが得意なのは前者である。膨大な医学テキストから学習したパターンを再現し、最も確率の高い選択肢を選ぶ。

AIが苦手なのは後者である。不確実な情報のもとで仮説を柔軟に修正し、「この所見があるなら、この可能性はどの程度上がるか」を直感的に判断する能力——**これは確率的パターンマッチングだけでは到達できない領域**なのだ。

### 臨床推論の本質

小児の発熱を診ている時、初診では単なるウイルス感染症と考えていたが、「夜中に手足が冷たくなった」という追加情報が得られた。

この瞬間、医師の頭の中では「髄膜炎」「敗血症」の可能性が一気に上がる。

**これがSCTが測る能力**——不確実な状況での柔軟な判断の修正——であり、現在のAIが最も苦手とする領域である。

---

## 診断における得意・不得意の偏り

診断精度においても、AIの性能は領域によって大きく異なる。

| 領域 | 感度 | 理由 |
|------|------|------|
| 皮膚科 | **85%** | 画像データが豊富、典型的パターンが明確 |
| 稀少疾患 | **52%** | 症例数が少なく、非典型的症例が多い |

この差は、**学習データの量と質がAIの性能を規定する**ことを意味する。

---

## ハルシネーション — もっともらしい嘘

AIの弱点として見過ごせないのが、ハルシネーション（幻覚）の問題である。

**医療領域におけるLLMのハルシネーション率：**
- **1.47%**

「たった1.47%」と思うかもしれない。しかし、ここで重要な数字がもう一つある。

**ハルシネーションのうち44%が臨床的に重大な誤りであった**という事実である。

1.47%のうちの44%——つまり、AIの出力全体のうち約**0.65%が「臨床的に重大な嘘」を含んでいる**ことになる。

**1日に100回AIに質問すれば、そのうち1回は重大な誤情報を受け取る計算だ。**

しかもその「嘘」は、もっともらしい文脈の中に自然に埋め込まれるため、注意深く読まなければ見逃してしまう。

---

## 得意・苦手マトリックス — 実務への落とし込み

以上の分析を踏まえ、臨床業務におけるAI活用を4段階に整理する。

| 活用レベル | 業務例 | 根拠 |
|:---:|:---|:---|
| **積極活用** | カルテ記載、文献検索、翻訳 | RCTで有効性実証済み。定型パターンの処理 |
| **補助的活用** | 鑑別リスト作成、患者教育資料の下書き | 有用だが人間の検証が必須 |
| **参考程度** | 臨床推論の壁打ち相手 | SCT研究で限界が判明。あくまで思考の補助 |
| **非推奨** | 投薬量の最終決定 | ハルシネーションリスクが許容不能 |

この表を見て気づくのは：
- **上に行くほど「パターン処理」の比重が高い**
- **下に行くほど「不確実性下の判断」の比重が高い**

### 判断の原則

AIに何を任せるかを判断する際の原則は明快だ。

**パターンが明確で、データが豊富で、出力の検証が容易なタスク** → AIに任せる価値が高い

**不確実性が高く、個別の文脈判断が求められ、誤りの影響が致命的なタスク** → 人間の判断が不可欠

---

## 「人間の代替」ではなく「人間の増幅」

「人間の代替」ではなく「人間の増幅」というのは、まさにこの原則を言い換えたものにほかならない。

- AIにカルテ記載を任せることで浮いた時間を、AIが苦手とする**臨床推論に充てる**
- AIが生成した鑑別リストを出発点として、自分の臨床経験と患児の文脈を加味して**最終判断を下す**

**定型業務をAIに委ね、人間にしかできない判断に集中する**——これが「増幅」の本質である。

---

## 検証コストという判断軸

AIを「増幅装置」として使いこなすためには、**AIの出力に対する「検証コスト」の感覚を持つ**ことが重要だ。

### 検証コストが低い（積極活用）

AIが生成したカルテの記載内容を確認するのに要する時間は、ゼロからカルテを書く時間よりはるかに短い。

### 検証コストが中程度（補助的活用）

AIが提示した鑑別診断リストの妥当性を検証するには、自分自身の臨床知識と経験を総動員する必要がある。

検証コストは高いが、それでもゼロから鑑別を考えるよりは効率的な場合がある。

### 検証コストが高い（非推奨）

AIが提案した投薬量が正しいかどうかを検証するコストは、自分で計算するコストとほぼ変わらない。

むしろ、AIの出力を「一旦信じてしまう」認知バイアス——いわゆる**オートメーションバイアス**——のリスクを考えると、最初から自分で計算した方が安全である。

### 判断基準

**AIの活用判断は、「AIにできるかどうか」ではなく「AIの出力を検証するコストが、自分で行うコストより低いかどうか」で考えるべき**なのである。

この視点を持つだけで、日々の業務におけるAI活用の意思決定は格段に明快になるだろう。

---

## まとめ

- **定型業務での圧倒的な強さ** - カルテ記載41秒/件短縮、文献検索67%時間短縮、翻訳BLEU 0.78
- **不確実性下の推論という決定的な弱点** - MCQで86～92%、SCTでは医学生にも劣る
- **領域による診断精度の偏り** - 皮膚科85% vs 稀少疾患52%。ハルシネーション率1.47%（うち44%が臨床的に重大）
- **「代替」ではなく「増幅」** - 定型業務をAIに委ね、浮いた時間を高度な臨床判断に
- **検証コストという判断軸** - 「AIにできるか」ではなく「検証コストが低いか」で判断する

---

### 参考文献

1. Van Buchem, M. M. et al. (2025). "Ambient AI Scribes for Clinical Documentation." *NEJM AI.*
2. Kung, T. H. et al. (2025). "Large Language Model Performance on Script Concordance Tests." *NEJM AI.*
3. Topol, E. J. et al. (2025). "AI-Generated Clinical Documentation Compared with Physician-Written Notes." *BMJ Open.*
4. Wang, L. et al. (2025). "AI-Assisted Systematic Literature Searching." *Journal of Clinical Epidemiology.*
